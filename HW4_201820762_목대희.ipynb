{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4_201820762_목대희",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeHee99/2022S-ML/blob/main/HW4_201820762_%EB%AA%A9%EB%8C%80%ED%9D%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HW4 :: DNN**\n",
        "## 과제 목표\n",
        "* 간단한 Three Layer Network를 구현하기\n",
        "* Pytorch를 사용하여 DNN 구현 후 학습과 테스트하기\n",
        "  \n",
        "  \n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "EXvAS7OZkg_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐  이번 과제는 bb에 코랩 링크, ipynb 파일만 업로드합니다(HW3와 동일하게).   \n",
        "⭐  작성한 코드에 **간단한 주석을 반드시 달아주세요**!  \n",
        "⭐  코딩할 부분을 제외하고는 수정하지 마세요. 수정 시 감점입니다."
      ],
      "metadata": {
        "id": "k5IhqPYwmnUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **문제 1 - Three Layer Network**\n",
        "```class Sigmoid```와 ```Affine```을 구현한 후 이 두 class를 사용하여 ```class ThreeLayerNet```를 완성하세요. \n",
        "* 코드 참고 : deep learning from scratch"
      ],
      "metadata": {
        "id": "OKfJ8-LiFOr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 1-1\n",
        "class sigmoid의 forward 함수를 구현하세요.  \n",
        "힌트) sigmoid 함수 식"
      ],
      "metadata": {
        "id": "FrrWMAJx6FUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QSI6QIBkCPWP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      #############################################\n",
        "      ################### 문제 1-1 #################\n",
        "      ############# sigmoid forward 구현 ###########\n",
        "      #############################################\n",
        "        # 한 줄로 구현\n",
        "        result = 1 / (1 + np.exp(-x))  # sigmoid 함수 식\n",
        "      #############################################\n",
        "      \n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 1-2\n",
        "class Affine의 forward 함수를 구현하세요.  \n",
        "힌트) affine 함수 식"
      ],
      "metadata": {
        "id": "1YcAkmtN6UXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine: # Affine은 Fully Connect를 의미합니다\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "      #############################################\n",
        "      ################### 문제 1-2 #################\n",
        "      ############# affine forward 구현 ############\n",
        "      #############################################\n",
        "        # 코드 작성\n",
        "        out = np.dot(x, self.params[0]) + self.params[1]  # affine 함수 식\n",
        "      #############################################\n",
        "      \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Ds05drVvG5O_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 1-3\n",
        "\n",
        "  각 layer의 parameter를 ```np.random.randn()``` 를 사용하여 초기화하세요.  \n",
        "  * 조건) ```class ThreeLayerNet```은 총 3개의 fully connected layer로 구성됩니다.\n",
        "  * 힌트) 차원을 잘 고려하세요. \n"
      ],
      "metadata": {
        "id": "aX3vaESK6jp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 1-4\n",
        "  문제1-1, 2에서 구현한 class를 사용하여 ThreeLayerNet의 layer를 구성하세요.\n",
        "  * 조건) ```class ThreeLayerNet```은 총 3개의 fully connected layer로 구성됩니다.\n",
        "  * 힌트) 차원을 잘 고려하세요."
      ],
      "metadata": {
        "id": "qoPlnkHg_18J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreeLayerNet:\n",
        "    def __init__(self, input_size, first_hidden_size, second_hidden_size, output_size):\n",
        "        I, H_1, H_2,O = input_size, first_hidden_size, second_hidden_size, output_size\n",
        "\n",
        "      #############################################\n",
        "      ################### 문제 1-3 #################\n",
        "      ######### parameter initialization ##########\n",
        "      #############################################\n",
        "        # 코드 작성\n",
        "        # 가중치(Weight)와 편향(bias) 초기화\n",
        "\n",
        "        # First Hidden Layer's parameter\n",
        "        W1 = np.random.randn(I, H_1)    # weight\n",
        "        b1 = np.random.randn(H_1)       # bias\n",
        "        \n",
        "        # Second Hidden Layer's parameter\n",
        "        W2 = np.random.randn(H_1, H_2)  # weight\n",
        "        b2 = np.random.randn(H_2)       # bias\n",
        "        \n",
        "        # Output Layer's parameter\n",
        "        W3 = np.random.randn(H_2, O)    # weight\n",
        "        b3 = np.random.randn(O)         # bias\n",
        "      #########################################\n",
        "        \n",
        "\n",
        "        self.layers = [\n",
        "        #############################################\n",
        "        ################### 문제 1-4 #################\n",
        "        ############### stack layers ################\n",
        "        #############################################          \n",
        "            # 코드 작성\n",
        "            # First Hidden Layer\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            \n",
        "            # Second Hidden Layer\n",
        "            Affine(W2, b2),\n",
        "            Sigmoid(),\n",
        "            \n",
        "            # Output Layer\n",
        "            Affine(W3, b3),\n",
        "            Sigmoid()\n",
        "        #############################################    \n",
        "        ]\n",
        "\n",
        "        # 모든 weight 를 담은 리스트 생성\n",
        "        self.params = []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VmHw4K5DG3uv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy data로 모델 실행해보기\n",
        "x = np.random.randn(784, 100)\n",
        "model = ThreeLayerNet(100, 50, 30, 10)\n",
        "s = model.predict(x)\n",
        "print(s)"
      ],
      "metadata": {
        "id": "SNI0xGraFAAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f23eba-b6d3-479b-fff1-1662c4a74875"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.78649858 0.87937615 0.00539969 ... 0.98565771 0.89283506 0.94140794]\n",
            " [0.94639794 0.96240739 0.07725007 ... 0.97915379 0.83598675 0.85521906]\n",
            " [0.3310725  0.82947842 0.02266564 ... 0.93491542 0.94593681 0.86521042]\n",
            " ...\n",
            " [0.89886956 0.97965421 0.05916871 ... 0.94316019 0.74116442 0.92084719]\n",
            " [0.98357388 0.90299749 0.02021612 ... 0.95929396 0.41786016 0.52102085]\n",
            " [0.40265612 0.98802823 0.59670472 ... 0.9958566  0.81226225 0.95223028]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FtZZAx7vovt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문제 2 - Implementing DNN using Pytorch\n",
        "문제 1에서는 Pytorch를 사용하지 않고 DNN을 구현해보았습니다.  \n",
        "문제 2에서는 Pytorch를 사용하여 DNN을 구현하고 MNIST 데이터로 분류 모델 학습을 진행합니다.\n",
        "* 코드 참고: pytorch 공식 튜토리얼"
      ],
      "metadata": {
        "id": "hOzYC0u5GfkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 importing\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "LKcI43VULpeQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Data**"
      ],
      "metadata": {
        "id": "WCbWy3jAMGuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # training data\n",
        "    download=True,\n",
        "    transform=ToTensor() # 이미지를 tensor로 변형\n",
        ")\n",
        "\n",
        "# Load test data\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # test data\n",
        "    download=True,\n",
        "    transform=ToTensor() # 이미지를 tensor로 변형\n",
        ")\n",
        "\n",
        "# data loader\n",
        "# train, test 각각의 data loader 생성\n",
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "D9DqIegtLnz9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check loaded data**\n",
        "train_loader를 사용하여 하나의 데이터를 로드한 후 이 데이터가 어떤 숫자의 데이터인지 이미지로 확인해봅니다."
      ],
      "metadata": {
        "id": "QtJVlNT9A_KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train feature와 label을 train_loader로부터 가져오기\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "IF90dcyJPhVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c199f0-ea8b-40c6-a783-403085ade7bd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([1, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지로 확인\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "00Wypwb2Pr-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "abbcb637-9c00-4653-97ed-73a5d2c5b588"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMVklEQVR4nO3dX6gc5R3G8efx3030InrsIWj8UxFFCtVykEKNWoqSeJMEoTQXJaXS40WFiL2o6IVCaZDSP/Sq4RTFpKSWQs7BUErVhmqam+oxpDFqqlZiTIg5DV6o5MKqv17sRE51d+ZkZ2Znze/7gWV3593d+TE5T96ZeXf2dUQIwJnvrK4LADAahB1IgrADSRB2IAnCDiRxzihXZptT/0DLIsL9ltfq2W2vtv0v22/Yvr/OZwFol4cdZ7d9tqTXJN0m6YikFyRtiIhXSt5Dzw60rI2e/UZJb0TEmxHxoaQ/SFpb4/MAtKhO2C+R9Pai50eKZf/H9rTtedvzNdYFoKbWT9BFxIykGYndeKBLdXr2o5JWLnp+abEMwBiqE/YXJF1t+0rb50n6jqSdzZQFoGlD78ZHxEe275H0lKSzJT0WES83VhmARg099DbUyjhmB1rXypdqAHxxEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DE0FM2A3UtW7astH3btm2l7atWrSptn5qaGth2+PDh0veeiWqF3fYhSe9L+ljSRxExeOsC6FQTPfs3I+JEA58DoEUcswNJ1A17SHra9ou2p/u9wPa07Xnb8zXXBaCGurvxN0XEUdtfkvSM7YMRsXvxCyJiRtKMJNmOmusDMKRaPXtEHC3uFyTNSbqxiaIANG/osNteZvuCU48l3S7pQFOFAWhWnd34SUlztk99zu8j4i+NVIUUrr322tL2tWvXlrYXf3sDTUxMDGxjnP00RMSbkr7aYC0AWsTQG5AEYQeSIOxAEoQdSIKwA0lwiSs6c/HFF5e2Vw2tnTx5slZ7NvTsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+xoVdllrFu3bi19b0T5DxvNzc2Vth88eLC0PRt6diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnF2tOrOO+8c2FZ1PXvVOPvmzZuHqikrenYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSMJVY5mNrswe3cowElVj5c8///zAtssuu6z0vVV/m+ecw9dE+omIvj+4X9mz237M9oLtA4uWXWj7GduvF/fLmywWQPOWshv/uKTVn1l2v6RdEXG1pF3FcwBjrDLsEbFb0rufWbxW0qnfFNoqaV3DdQFo2LAHPZMRcax4/I6kyUEvtD0taXrI9QBoSO0zHBERZSfeImJG0ozECTqgS8MOvR23vUKSivuF5koC0IZhw75T0sbi8UZJTzZTDoC2VO7G235C0q2SJmwfkfSQpEck/dH2XZLekvTtNovE+NqyZUtpe9lYetX86ydOnBiqJvRXGfaI2DCg6VsN1wKgRXxdFkiCsANJEHYgCcIOJEHYgSS4RhClHnzwwdL2devKL4sou0y1amhtzZo1pe04PfTsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+woVTWOXucy1VtuuaX0vQcPHixtx+mhZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnT65qyuWJiYnS9qpplWdnZwe2MY4+WvTsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zJPffcc6XtZVMuS9XXs+/Zs+e0a0I7Knt224/ZXrB9YNGyh20ftb2vuN3RbpkA6lrKbvzjklb3Wf6riLi+uP252bIANK0y7BGxW9K7I6gFQIvqnKC7x/b+Yjd/+aAX2Z62PW97vsa6ANQ0bNh/I+kqSddLOibpF4NeGBEzETEVEVNDrgtAA4YKe0Qcj4iPI+ITSb+VdGOzZQFo2lBht71i0dP1kg4Mei2A8VA5zm77CUm3SpqwfUTSQ5JutX29pJB0SNLdLdaIGtavX1/afs0115S2V12vPjc3V6sdo1MZ9ojY0Gfxoy3UAqBFfF0WSIKwA0kQdiAJwg4kQdiBJLjE9Qxw+eWXD2zbsmVL6XvrTLksSdu3by9tP3nyZGk7RoeeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9DLBq1aqBbRdddFHpe6suYd28eXNpO5ewfnHQswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEq4aZ210ZfboVnYGufnmm0vbn3322YFtVf++e/fuLW1fs2ZNaXvV9e4YvYjo+yMF9OxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATXs38BrFu3rrS9bCy9apydcfQ8Knt22ytt/832K7Zftr2pWH6h7Wdsv17cL2+/XADDWspu/EeSfhQR10n6uqQf2r5O0v2SdkXE1ZJ2Fc8BjKnKsEfEsYjYWzx+X9Krki6RtFbS1uJlWyWV72sC6NRpHbPbvkLSDZL+IWkyIo4VTe9ImhzwnmlJ08OXCKAJSz4bb/t8STsk3RsR7y1ui95ZoL5ngiJiJiKmImKqVqUAallS2G2fq17Qt0fEbLH4uO0VRfsKSQvtlAigCZW78e7N6fuopFcj4peLmnZK2ijpkeL+yVYqTGD16tWl7Zs2bSptP+uswf9n79ixo/S9DK3lsZRj9m9I+q6kl2zvK5Y9oF7I/2j7LklvSfp2OyUCaEJl2CNij6S+F8NL+laz5QBoC1+XBZIg7EAShB1IgrADSRB2IAkucR0DdS5hlaSFhcHfZ7rvvvuGqglnHnp2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYx0PvJgOHbt2/fPrDt8OHDQ9WEMw89O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTj7GKi6Xr2qfXZ2trQdkOjZgTQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJV43h2l4paZukSUkhaSYifm37YUk/kPSf4qUPRMSfKz6rfGUAaouIvj+AsJSwr5C0IiL22r5A0ouS1qk3H/sHEfHzpRZB2IH2DQr7UuZnPybpWPH4fduvSrqk2fIAtO20jtltXyHpBkn/KBbdY3u/7cdsLx/wnmnb87bna1UKoJbK3fhPX2ifL+k5ST+NiFnbk5JOqHcc/xP1dvW/X/EZ7MYDLRv6mF2SbJ8r6U+SnoqIX/Zpv0LSnyLiKxWfQ9iBlg0Ke+VuvHs/bfqopFcXB704cXfKekkH6hYJoD1LORt/k6S/S3pJ0ifF4gckbZB0vXq78Yck3V2czCv7LHp2oGW1duObQtiB9g29Gw/gzEDYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYtRTNp+Q9Nai5xPFsnE0rrWNa10StQ2rydouH9Qw0uvZP7dyez4ipjoroMS41jaudUnUNqxR1cZuPJAEYQeS6DrsMx2vv8y41jaudUnUNqyR1NbpMTuA0em6ZwcwIoQdSKKTsNtebftftt+wfX8XNQxi+5Dtl2zv63p+umIOvQXbBxYtu9D2M7ZfL+77zrHXUW0P2z5abLt9tu/oqLaVtv9m+xXbL9veVCzvdNuV1DWS7TbyY3bbZ0t6TdJtko5IekHShoh4ZaSFDGD7kKSpiOj8Cxi2b5b0gaRtp6bWsv0zSe9GxCPFf5TLI+LHY1LbwzrNabxbqm3QNOPfU4fbrsnpz4fRRc9+o6Q3IuLNiPhQ0h8kre2gjrEXEbslvfuZxWslbS0eb1Xvj2XkBtQ2FiLiWETsLR6/L+nUNOOdbruSukaii7BfIuntRc+PaLzmew9JT9t+0fZ018X0Mblomq13JE12WUwfldN4j9Jnphkfm203zPTndXGC7vNuioivSVoj6YfF7upYit4x2DiNnf5G0lXqzQF4TNIvuiymmGZ8h6R7I+K9xW1dbrs+dY1ku3UR9qOSVi56fmmxbCxExNHifkHSnHqHHePk+KkZdIv7hY7r+VREHI+IjyPiE0m/VYfbrphmfIek7RExWyzufNv1q2tU262LsL8g6WrbV9o+T9J3JO3soI7Psb2sOHEi28sk3a7xm4p6p6SNxeONkp7ssJb/My7TeA+aZlwdb7vOpz+PiJHfJN2h3hn5f0t6sIsaBtT1ZUn/LG4vd12bpCfU2637r3rnNu6SdJGkXZJel/RXSReOUW2/U29q7/3qBWtFR7XdpN4u+n5J+4rbHV1vu5K6RrLd+LoskAQn6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8BQmPtWHxAJGIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 2-1\n",
        "4개의 linear layer와 3개의 ReLU layer를 가진 네트워크를 구성하세요.\n",
        "\n"
      ],
      "metadata": {
        "id": "BB_Qe54pB8_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 2-2\n",
        "forward 함수의 빈칸을 구현하세요."
      ],
      "metadata": {
        "id": "mK5ukeeECYJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten() # 28x28 이미지를 784 픽셀 값의 배열로 변경\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            \n",
        "            nn.Linear(in_features=28*28, out_features=512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #############################################\n",
        "            ################### 문제 2-1 #################\n",
        "            # 4개의 linear layer와 3개의 ReLU layer를 구성하세요\n",
        "            # (위 Linear 포함 4개, ReLU layer 포함 3개를 의미)\n",
        "            #############################################\n",
        "            \n",
        "            # 시작 차원, 끝 차원 잘 고려하여 작성하기\n",
        "            # 중간 차원은 임의로 설정 가능\n",
        "            nn.Linear(in_features=512, out_features=256),  # 두번째 linear layer\n",
        "            nn.ReLU(),                                     # 두번째 ReLU layer\n",
        "\n",
        "            nn.Linear(in_features=256, out_features=128),   # 세번째 linear layer\n",
        "            nn.ReLU(),                                     # 세번째 ReLU layer\n",
        "\n",
        "            nn.Linear(in_features=128, out_features=10)     # 네번째 linear layer\n",
        "\n",
        "            #############################################\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #############################################\n",
        "        ################### 문제 2-2 #################\n",
        "        # forward 함수 구현\n",
        "        #############################################\n",
        "        # 코드 작성\n",
        "        x = self.flatten(x) # 28x28 이미지를 784 픽셀 값의 배열로 변경\n",
        "        logits = self.linear_relu_stack(x) # 위에서 정의한 Linear와 ReLU layer를 거쳐 최종적으로 10개(0~9)의 확률적 출력을 만듦\n",
        "        #############################################\n",
        "        return logits # forward 결과 저장"
      ],
      "metadata": {
        "id": "zme9j_4hMiA2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cpu OR gpu 설정\n",
        "# gpu가 있을 경우, device로 cuda를 사용함\n",
        "# colab에서 '런타임 유형 변경'을 하면 gpu 사용할 수 있음\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "kfBLbfwUJgtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c798e7-5588-41aa-e75b-d8444e5331f0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device) # device로 Network 전송\n",
        "print(model) # 모델 구조 확인"
      ],
      "metadata": {
        "id": "ifGukRqQOUyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc6458c-d3db-4f44-b821-77878bb67ba9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞에서 출력해보았던 train_features[0](1개의 데이터)에 대해서 모델 학습 결과 확인해보기\n",
        "logits = model(train_features[0]) # 일부 백그라운드 연산들과 함께 모델의 forward 를 실행 \n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "id": "MdOf0Rd1VEi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8d8290-8873-41f3-84bc-60a6ed28bb3c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train the Network** \n",
        "epoch과 batch를 활용하여 모델을 학습시켜 봅시다."
      ],
      "metadata": {
        "id": "J5G6rO77V7OR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제 2-3\n",
        "모델의 forward, backward, optimize 하는 부분을 주어진 칸에 구현하세요."
      ],
      "metadata": {
        "id": "n7RwQwKDCjOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "y7jdCHQphsIO"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter 설정\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # optimizer\n",
        "\n",
        "n_epoch = 3 # the number of epochs\n",
        "n_batch = 32 # the number of batches"
      ],
      "metadata": {
        "id": "XGXKm0pHhnoO"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader 설정하기\n",
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size=n_batch, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=n_batch, shuffle=True)"
      ],
      "metadata": {
        "id": "s3viP29EipLg"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        # input data 가져오기\n",
        "        # data 는 [inputs, labels]로 구성된 리스트\n",
        "        inputs, labels = data\n",
        "\n",
        "        # optimizer의 파라미터 gradient를 0으로 설정\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #############################################\n",
        "        ################### 문제 2-3 #################\n",
        "        # forward, backward, optimize \n",
        "        #############################################\n",
        "        # 코드 작성\n",
        "        # Forward\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        #############################################\n",
        "\n",
        "        # loss 출력\n",
        "        running_loss += loss.item()\n",
        "        if i % n_batch == 0:    # print every n_batch mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / n_batch:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "m6ByMb4whKFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3463f4ef-37fb-45b9-ba1c-e998bfa2b7a4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 0.009\n",
            "[1,    33] loss: 0.244\n",
            "[1,    65] loss: 0.261\n",
            "[1,    97] loss: 0.255\n",
            "[1,   129] loss: 0.259\n",
            "[1,   161] loss: 0.248\n",
            "[1,   193] loss: 0.245\n",
            "[1,   225] loss: 0.243\n",
            "[1,   257] loss: 0.289\n",
            "[1,   289] loss: 0.283\n",
            "[1,   321] loss: 0.262\n",
            "[1,   353] loss: 0.266\n",
            "[1,   385] loss: 0.250\n",
            "[1,   417] loss: 0.227\n",
            "[1,   449] loss: 0.289\n",
            "[1,   481] loss: 0.220\n",
            "[1,   513] loss: 0.275\n",
            "[1,   545] loss: 0.274\n",
            "[1,   577] loss: 0.243\n",
            "[1,   609] loss: 0.285\n",
            "[1,   641] loss: 0.271\n",
            "[1,   673] loss: 0.252\n",
            "[1,   705] loss: 0.233\n",
            "[1,   737] loss: 0.241\n",
            "[1,   769] loss: 0.281\n",
            "[1,   801] loss: 0.226\n",
            "[1,   833] loss: 0.308\n",
            "[1,   865] loss: 0.237\n",
            "[1,   897] loss: 0.258\n",
            "[1,   929] loss: 0.262\n",
            "[1,   961] loss: 0.234\n",
            "[1,   993] loss: 0.254\n",
            "[1,  1025] loss: 0.238\n",
            "[1,  1057] loss: 0.269\n",
            "[1,  1089] loss: 0.268\n",
            "[1,  1121] loss: 0.289\n",
            "[1,  1153] loss: 0.247\n",
            "[1,  1185] loss: 0.256\n",
            "[1,  1217] loss: 0.291\n",
            "[1,  1249] loss: 0.278\n",
            "[1,  1281] loss: 0.276\n",
            "[1,  1313] loss: 0.247\n",
            "[1,  1345] loss: 0.209\n",
            "[1,  1377] loss: 0.242\n",
            "[1,  1409] loss: 0.219\n",
            "[1,  1441] loss: 0.261\n",
            "[1,  1473] loss: 0.262\n",
            "[1,  1505] loss: 0.278\n",
            "[1,  1537] loss: 0.228\n",
            "[1,  1569] loss: 0.227\n",
            "[1,  1601] loss: 0.243\n",
            "[1,  1633] loss: 0.226\n",
            "[1,  1665] loss: 0.296\n",
            "[1,  1697] loss: 0.248\n",
            "[1,  1729] loss: 0.230\n",
            "[1,  1761] loss: 0.233\n",
            "[1,  1793] loss: 0.229\n",
            "[1,  1825] loss: 0.265\n",
            "[1,  1857] loss: 0.220\n",
            "[2,     1] loss: 0.005\n",
            "[2,    33] loss: 0.200\n",
            "[2,    65] loss: 0.243\n",
            "[2,    97] loss: 0.227\n",
            "[2,   129] loss: 0.220\n",
            "[2,   161] loss: 0.231\n",
            "[2,   193] loss: 0.197\n",
            "[2,   225] loss: 0.271\n",
            "[2,   257] loss: 0.238\n",
            "[2,   289] loss: 0.214\n",
            "[2,   321] loss: 0.232\n",
            "[2,   353] loss: 0.170\n",
            "[2,   385] loss: 0.237\n",
            "[2,   417] loss: 0.200\n",
            "[2,   449] loss: 0.252\n",
            "[2,   481] loss: 0.230\n",
            "[2,   513] loss: 0.255\n",
            "[2,   545] loss: 0.226\n",
            "[2,   577] loss: 0.214\n",
            "[2,   609] loss: 0.216\n",
            "[2,   641] loss: 0.194\n",
            "[2,   673] loss: 0.179\n",
            "[2,   705] loss: 0.194\n",
            "[2,   737] loss: 0.233\n",
            "[2,   769] loss: 0.251\n",
            "[2,   801] loss: 0.198\n",
            "[2,   833] loss: 0.203\n",
            "[2,   865] loss: 0.215\n",
            "[2,   897] loss: 0.186\n",
            "[2,   929] loss: 0.231\n",
            "[2,   961] loss: 0.210\n",
            "[2,   993] loss: 0.224\n",
            "[2,  1025] loss: 0.205\n",
            "[2,  1057] loss: 0.220\n",
            "[2,  1089] loss: 0.184\n",
            "[2,  1121] loss: 0.232\n",
            "[2,  1153] loss: 0.155\n",
            "[2,  1185] loss: 0.186\n",
            "[2,  1217] loss: 0.164\n",
            "[2,  1249] loss: 0.218\n",
            "[2,  1281] loss: 0.179\n",
            "[2,  1313] loss: 0.194\n",
            "[2,  1345] loss: 0.188\n",
            "[2,  1377] loss: 0.208\n",
            "[2,  1409] loss: 0.204\n",
            "[2,  1441] loss: 0.195\n",
            "[2,  1473] loss: 0.203\n",
            "[2,  1505] loss: 0.219\n",
            "[2,  1537] loss: 0.197\n",
            "[2,  1569] loss: 0.209\n",
            "[2,  1601] loss: 0.165\n",
            "[2,  1633] loss: 0.226\n",
            "[2,  1665] loss: 0.171\n",
            "[2,  1697] loss: 0.239\n",
            "[2,  1729] loss: 0.203\n",
            "[2,  1761] loss: 0.207\n",
            "[2,  1793] loss: 0.176\n",
            "[2,  1825] loss: 0.173\n",
            "[2,  1857] loss: 0.177\n",
            "[3,     1] loss: 0.009\n",
            "[3,    33] loss: 0.182\n",
            "[3,    65] loss: 0.181\n",
            "[3,    97] loss: 0.193\n",
            "[3,   129] loss: 0.191\n",
            "[3,   161] loss: 0.195\n",
            "[3,   193] loss: 0.183\n",
            "[3,   225] loss: 0.174\n",
            "[3,   257] loss: 0.183\n",
            "[3,   289] loss: 0.200\n",
            "[3,   321] loss: 0.187\n",
            "[3,   353] loss: 0.166\n",
            "[3,   385] loss: 0.215\n",
            "[3,   417] loss: 0.189\n",
            "[3,   449] loss: 0.181\n",
            "[3,   481] loss: 0.199\n",
            "[3,   513] loss: 0.214\n",
            "[3,   545] loss: 0.205\n",
            "[3,   577] loss: 0.184\n",
            "[3,   609] loss: 0.161\n",
            "[3,   641] loss: 0.163\n",
            "[3,   673] loss: 0.178\n",
            "[3,   705] loss: 0.153\n",
            "[3,   737] loss: 0.176\n",
            "[3,   769] loss: 0.185\n",
            "[3,   801] loss: 0.177\n",
            "[3,   833] loss: 0.140\n",
            "[3,   865] loss: 0.150\n",
            "[3,   897] loss: 0.156\n",
            "[3,   929] loss: 0.147\n",
            "[3,   961] loss: 0.206\n",
            "[3,   993] loss: 0.132\n",
            "[3,  1025] loss: 0.160\n",
            "[3,  1057] loss: 0.156\n",
            "[3,  1089] loss: 0.159\n",
            "[3,  1121] loss: 0.185\n",
            "[3,  1153] loss: 0.182\n",
            "[3,  1185] loss: 0.162\n",
            "[3,  1217] loss: 0.172\n",
            "[3,  1249] loss: 0.163\n",
            "[3,  1281] loss: 0.167\n",
            "[3,  1313] loss: 0.163\n",
            "[3,  1345] loss: 0.160\n",
            "[3,  1377] loss: 0.170\n",
            "[3,  1409] loss: 0.191\n",
            "[3,  1441] loss: 0.158\n",
            "[3,  1473] loss: 0.169\n",
            "[3,  1505] loss: 0.151\n",
            "[3,  1537] loss: 0.195\n",
            "[3,  1569] loss: 0.192\n",
            "[3,  1601] loss: 0.175\n",
            "[3,  1633] loss: 0.158\n",
            "[3,  1665] loss: 0.188\n",
            "[3,  1697] loss: 0.174\n",
            "[3,  1729] loss: 0.186\n",
            "[3,  1761] loss: 0.145\n",
            "[3,  1793] loss: 0.168\n",
            "[3,  1825] loss: 0.151\n",
            "[3,  1857] loss: 0.153\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test the Network**"
      ],
      "metadata": {
        "id": "XnqNJjGki4JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test feature와 label을 test_loader로부터 가져오기\n",
        "test_features, test_labels = next(iter(test_loader))\n",
        "print(f\"Feature batch shape: {test_features.size()}\")\n",
        "print(f\"Labels batch shape: {test_labels.size()}\")"
      ],
      "metadata": {
        "id": "CO1AMDEAjGrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9a43cd-3ae5-442e-e8c0-69fe1e3d8de4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([32, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1개 이미지 확인해보기\n",
        "\n",
        "logits = model(test_features[0]) # 일부 백그라운드 연산들과 함께 모델의 forward 를 실행 \n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "\n",
        "\n",
        "img = test_features[0].squeeze()\n",
        "label = test_labels[0]\n",
        "\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "bgC3vITkjWdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "be0dabef-a078-4ef1-9f70-90c0baa93197"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([4])\n",
            "Label: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 test data에 대한 결과 확인\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad(): # 모델을 학습하는 것이 아니므로 gradient 계산을 할 필요가 없음\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "RE_tglcsmmRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c396eea6-a296-4d20-ba36-4d3884b25be7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 95 %\n"
          ]
        }
      ]
    }
  ]
}